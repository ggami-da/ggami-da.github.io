---
layout: post
title:  "[Casual Inference] Graph Model - DAG와 확률 분포"
date:   2024-11-25
last_modified_at: 2024-11-25
categories: [Casual Inference]
tags: [Casual Inference, Data]
---

{: .box-success}
주로 분석할 때 많이 사용하는 모델 이론을 간략히 정리합니다.


# 그래프 이론 (DAG와 확률분포)

# DAG와 확률분포

DAG가 어떻게 확률 분포에 대한 정보를 코드화하는지 이해하는 것이고, 둘째는 DAG로부터 **결합 분포**를 분해하는 것을 알아야 합니다.

DAG는 

- 어떤 변수들이 서로 독립인지
- 어떤 변수들이 서로 조건부 독립인지
- 또한 결합 분포를 인수분해하고 간소화하는 방법을 알려줍니다

![img.png](../../../../img/graph-model(4).png)

위의 예시를 본다면 4개의 노드 혹은 변수 집합인 A, B, C, D가 있습니다.  
이는 단일 지향 경로만 있는 DAG입니다. 이를 바탕으로 어떻게 결합 분포에 대한 정보를 나타내는지 확인을 해봅시다.  
위의 DAG를 통해서 아래의 정보를 알 수 있습니다.  

1. $$P(C|A, B, D) = P(C)$$
- 첫 번째로 C는 모든 변수와 독립이라는 것을 알려줍니다.   
- 이유는 C는 단독으로 홀로 자리를 잡고 있기 때문입니다. 따라서 C는 어떤 분포를 가지는 변수이지만 다른 변수에 영향을 미치지 않으며 다른 변수의 영향을 받지 않기 때문에 C는 완전 독립입니다.

2. $$P(B|A, C, D) = P(B|A)$$
- B는 A의 직접적인 영향을 받으며 D의 간접적인 영향을 받습니다. D가 A에 영향을 주고, A가 다시 B에 영향을 주기 때문에 이 경우 실질적으로 A가 B에 직접적인 영향을 주기에 가장 중요한 것은 A 뿐이라는 것을 알 수 있습니다. 
- 따라서 $A$를 알면 $B$의 확률에 대해 알아야 할 모든 것을 알 수 있고 D는 A에 영향을 미쳤지만, 우리는 A에 대한 조건화를 하고 있기 때문에 A를 이미 알고 있으므로 추가적인 정보를 제공하지 않게 되는 것입니다.

3. $$P(B|D) ≠ P(B)$$
- B와 D는 marginally dependent라는 것을 알 수 있습니다. 
- 이는 D가 A에 영향을 미치고 A가 B에 영향을 미치기 때문이며 D는 B와 관련이 있어야 합니다. 즉, 독립적이지 않고 marginally dependent이라는 것입니다.

4. $$P(D|A,B,C) = P(D|A)$$
- 비슷한 이유로 이를 단순히 A$를 조건으로 하는 D의 확률로 간단화할 수 있습니다. 
- B 또한 D와 관련이 있지만, 오직 A를 통해서만 관련이 있기 때문에 D는 A에 영향을 미치고, A는 B에 영향을 미치지만, A에 대한 조건화를 하면 D에 대해 실제로 알아야 할 모든 것을 알 수 있습니다.

![img.png](../../../../img/graph-model(5).png)

또 다른 예시를 본다면 다음과 같습니다.

1. $$P(A|B, C, D) = P(A|D)$$ 이며 $$A \perp\!\!\perp B, C |D$$
- A에 영향을 미치는 유일한 요소가 D이기 떄문에 D에 대해서 조건화를 한다면 B, C와는 독립이게 됩니다.

2. $$P(D|A,B,C) = P(D|A,B)$$ 이며 $$D \perp\!\!\perp C |B$$
- C를 조건에서 제외할 수 있습니다. 하지만 여전히 A와 B에 대한 조건화가 필요합니다. 
- 즉, D는 B를 조건으로 할 때 C와 독립하다는 것을 의미합니다. B에 대한 조건화를 하면, C에는 A에 대한 정보가 없기 때문입니다.
- 따라서 B에 대한 조건화를 하지 않았다면 C가 간접적으로 A에 대한 정보를 제공할 수 있지만 B에 대한 조건화를 하는 한, C는 혼자 떨어져 있기 때문에 유용한 정보를 제공하지 않게 됩니다.

# DAG와 결합분포 분해 (Decomposition)

자 이제 다음으로 결합분포의 분해를 한번 살펴봅시다.  
분해는 부모가 없는 노드(=루트)부터 시작하게 됩니다.  
그 다음 조상 계열을 따라 아래로 이동하면서 항상 부모에 대한 조건화를 수행하게 됩니다.  

![img.png](../../../../img/graph-model(4).png)

위에서 예제는 앞서 사용한 예제를 다시 사용하겠습니다.  
위의 DAG를 본다면 부모 집합만 순차적으로 조건화를 하여 결합 분포를 분해할 수 있습니다.  

1. 루트를 찾는다면 위의 예시에서 루트는 C, D 2개입니다. 이후 C의 확률과 D의 확률을 곱합니다.
2. 다음으로 루트의 자식 노드를 찾습니다. C는 자식 노드가 없으므로 C는 완료되고 D는 자식 노드가 하나 있는데, 바로 A입니다. 따라서 D를 조건으로 하는 A의 확률을 곱합니다.
3. 다음으로 A의 자식 노드를 찾으면 B가 유일하며 A를 조건으로 하는 B의 확률을 곱합니다.
결과적으로 이 DAG는 A, B, C, D의 결합 분포를 $$P(A, B, C, D) = P(C)P(D)P(A|D)P(B|A)$$ 로 쓸 수 있습니다.  
다른 예시를 보겠습니다.  

![img.png](../../../../img/graph-model(5).png)


1. 루트를 찾는다면 D만 존재합니다.
2. 다음으로 자식 노드를 찾으면 A와 B 두 개가 있습니다. 그런 다음 D를 조건으로 하는 A의 확률과 D를 조건으로 하는 B의 확률을 곱합니다.
3. 다음으로 A와 B의 자식 노드를 찾는다. A는 자식 노드가 없으므로 실제로 이 노드 또는 변수는 완료되었다고 볼 수 있습니다. 하지만 B는 자식 노드가 하나 있으며, 바로 C입니다. 따라서 B를 조건으로 하는 C의 확률을 곱할 수 있습니다.

결과적으로 이 DAG는 A, B, C, D의 결합 분포를 $$P(A, B, C, D) = P(D)P(A|D)P(B|D)P(C|B)$$로 쓸 수 있습니다.  
이처럼 **DAG가 결합 분포를 어떻게 분해할 수 있는지를 직접적으로 알려준다는 것**을 알 수 있습니다.  
따라서 DAG와 분포 사이에는 이러한 종류의 호환성(Compatible)이 있습니다.  
이 특정 DAG로 돌아가면 특정 인수 분해가 허용된다는 것을 알 수 있고 특정 확률 분해를 이 특정 DAG와 호환되는 것으로 생각할 수 있습니다.

![img.png](../../../../img/graph-model(6).png)

다만 특정 확률 변수가 반드시 고유한 DAG를 의미하지 않을 수 있습니다.  
간단하게 두개의 DAG가 있습니다. 서로 비슷한 것 같지만 첫 번째는 A가 B에 영향을 주고 두 번째는 B가 A에 영향을 주고 있음을 알 수 있습니다. 또한 A와 B가 종속적인 것도 알 수 있습니다.  

예를 들어서  “A와 B의 확률이 A의 확률과 B의 확률의 곱과 같지 않다고 말했다고 가정합니다.” ⇒ 다른 말로는 A와 B가 종속적이거나 독립적이지 않다는 것을 먼저 알고 있다고 가정해본다면 이후 DAG를 그리게 되면 첫 번째 혹은 두 번째 그림을 모두 그릴 수 있습니다.  

즉 두 개의 DAG 모두 말한 부분과 호환은 되지만 실제 어떤 것이 올바른지 알 수 없습니다.  
하지만 DAG부터 시작을 했다면 명확히 확률에 대해서 말을 할 수 있었을 것입니다.  