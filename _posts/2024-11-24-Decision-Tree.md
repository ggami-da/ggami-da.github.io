---
layout: post
title:  "[ML] Decision Tree"
date:   2024-11-23
last_modified_at: 2024-11-23
categories: [ML]
tags: [ML, Data]
---

{: .box-success}
주로 분석할 때 많이 사용하는 모델 이론을 간략히 정리합니다.

# **의사결정나무**

**의사결정나무(Decision Tree)**는 다양한 규칙을 순차적으로 적용하여 독립 변수 공간을 분할하는 강력한 모델링 기법입니다. 이는 복잡한 데이터에서 패턴을 식별하고 예측을 가능하게 하며, 분류(Classification)와 회귀(Regression) 문제 모두에 활용될 수 있습니다. 이러한 의사결정나무는 여러 알고리즘을 통해 발전해 왔으며, 그중 주요 알고리즘으로 **ID3, C4.5, C5.0, 그리고 CART**가 있습니다.

- **ID3 (Iterative Dichotomiser 3):** J. Ross Quinlan이 1980년대 초에 개발한 알고리즘으로, 정보 이득(Information Gain)을 활용하여 데이터를 분할합니다. ID3는 범주형 데이터를 처리하는 데 적합하지만, 연속형 데이터 처리에는 제약이 있으며, 과적합(overfitting) 문제도 존재할 수 있습니다.
- **C4.5:** ID3를 개선한 C4.5는 퀸란에 의해 1993년에 발표되었습니다. 이 알고리즘은 이득률(Gain Ratio)을 사용하여 정보 이득의 편향된 본성을 보완하고, 연속형 데이터와 누락값을 보다 효과적으로 처리할 수 있게 되었습니다. 이러한 개선 덕분에 C4.5는 큰 인기를 끌었습니다.
- **C5.0:** C5.0은 C4.5의 상용 버전으로, 더욱 빠르고 메모리 효율적인 특성을 가지고 있습니다. 이 알고리즘은 트리의 복잡성을 줄이고 정확성을 높이는 데 중점을 둡니다. C5.0은 대규모 데이터셋에도 잘 적용될 수 있습니다.
- **CART (Classification And Regression Tree):** 1984년에 Breiman et al.에 의해 발표된 이 알고리즘은 의사결정나무를 통한 분류와 회귀 분석에 사용하는 기법입니다. CART는 지니 지수(Gini Index)를 분할 기준으로 사용하며, 다양한 방식의 가지치기(pruning)를 통해 모델의 복잡성을 제어합니다. CART는 직관적이고 해석이 쉬워 여전히 많은 데이터 분석가들에게 널리 사용되고 있습니다.

## **왜 CART가 주로 사용되나?**

Python과 R에서 많이 사용되는 이유 중 하나 **단순함, 효율성, 범용성**이라는 세 가지 요소를 충족시키기 떄문입니다. 
- **오픈소스 라이선스**: CART는 공개 알고리즘이라 구현과 배포가 자유롭습니다.
- **단순성 및 효율성**: 이진 분할 기반의 단순 구조로 대규모 데이터셋에 적합하고 빠릅니다.
- **범용성**: 분류와 회귀 모두 지원하며, 다양한 문제에서 활용 가능합니다.
- **유연성**: 사용자가 수정하고 확장하기 쉬운 구조입니다.
- **표준화된 생태계**: 대부분의 머신러닝 패키지와 연구에서 표준적으로 채택되어 있습니다.

## 의사결정나무를 이용한 분류학습

의사결정나무를 이용한 분류법은 다음과 같습니다.
1. 여러가지 독립 변수 중 **하나의 독립 변수를 선택**하고 그 독립 변수에 대한 기준값(threshold)을 정한다. ⇒ 이를 분류 규칙이라고 합니다.
2. 전체 학습 데이터 집합(부모 노드)을 해당 독립 변수의 값이 기준값보다 작은 데이터 그룹(자식 노드 1)과 해당 독립 변수의 값이 기준값보다 큰 데이터 그룹(자식 노드 2)으로 나눕니다.
3. 각각의 자식 노드에 대해 1~2의 단계를 반복하여 하위의 자식 노드를 만듭니다. 
단, 자식 노드에 한가지 클래스의 데이터만 존재한다면 더 이상 자식 노드를 나누지 않고 중지합니다.

이렇게 자식 노드 나누기를 연속적으로 적용하면 노드가 계속 증가하는 나무(tree)와 같은 형태로 표현할 수 있습니다.

![image.png](/ggami-da.github.io/_posts/img//decision-tree(1).png)
![image.png](/ggami-da.github.io/_posts/img//decision-tree(2).png)
![image.png](/ggami-da.github.io/_posts/img//decision-tree(3).png)

{: .box-success}
plot을 기반으로 해석을 하고자 하는 경우에는 max_depth를 조정하여 도메인적으로 타당한 수준을 확인하여 나무를 생성하는 경우도 있습니다.

## 의사결정나무를 사용한 분류예측

의사결정나무에 전체 훈련 데이터를 모두 적용해 보면 각 데이터는 특정한 노드를 타고 내려가게 됩니다.  
각 노드는 그 노드를 선택한 데이터 집합을 가집니다. 이 때 노드에 속한 데이터의 클래스의 비율을 구하여 이를 그 노드의 조건부 확률 분포 $$P(Y=k|X)_{\text{node}}$$ 라고 정의합니다.

$$
 P(Y=k|X)_
{\text{node}} \approx \dfrac{N_{\text{node},k}}{N_{\text{node}}} 
$$

훈련 데이터 $$X_{\text{test}}$$ 의 클래스를 예측할 때는 가장 상위의 노드 부터 분류 규칙을 차례대로 적용하여 마지막에 도달하는 노드의 조건부 확률 분포를 이용하여 클래스를 예측하게 됩니다.

$$
\hat{Y} = \text{arg}\max_k P(Y=k|X_{\text{test}})_{\text{last node}} 
$$

## 분류규칙을 정하는 방법

분류 규칙을 정하는 방법은 부모 노드와 자식 노드 간의 엔트로피를 가장 낮게 만드는 최상의 독립 변수와 기준값을 찾는 것입니다.  
이러한 기준을 정량화한 것이 **`정보획득량(information gain)입니다.`**기본적으로 모든 독립 변수와 모든 가능한 기준값에 대해 정보 획득량을 구하여 가장 정보 획득량이 큰 독립 변수와 기준값을 선택하게 됩니다.

## 정보획득량

정보획득량(information gain)는 $$X$$ 라는 조건에 의해 확률 변수 $$Y$$ 의 엔트로피가 얼마나 감소 하였는가를 나타내는 값입니다.  
다음처럼 $$Y$$ 의 **`엔트로피`**에서 $$X$$ 에 대한 $$Y$$ 의 조건부 엔트로피를 뺀 값으로 정의됩니다.  
엔트로피는 𝐻[]기호로 표기하며 다음과 같이 변수의 특성에 따라 계산이 달라집니다.  

{: .box-success}
⛔ 확률이 얼마나 골고루 분포가 되었는지, 아니면 한쪽에 집중이 되었는지 나타내는 것이 엔트로피며 골고루 퍼져있다면 엔트로피는 높아지고 한쪽에 몰려있다면 엔트로피는 줄어듭니다.  

확률변수 𝑌 가 카테고리분포와 같은 이산확률변수이면 다음처럼 정의합니다.

$$
\begin{align}
H[Y] = -\sum_{k=1}^K p(y_k) \log_2 p(y_k)
\tag{10.1.1}
\end{align}

$$

확률변수 𝑌가 정규분포와 같은 연속확률변수이면 다음처럼 정의합니다.

$$
\begin{align}H[Y] = -\int_{-\infty}^{\infty} p(y) \log_2 p(y) \; dy\tag{10.1.2}\end{align}
$$

$$
Information Gain[Y,X] = H[Y] - H[Y|X] 
$$

CART(Classification and Regression Trees)에서 사용되는 대표적인 분류 규칙 평가 척도는 사실 지니계수입니다.  
이는 디폴트로 설정이 되어있고 파라메터를 엔트로피로 변경할 수 있습니다.  
지니 계수는 엔트로피와 비슷한 개념이지만 계산이 더 단순합니다.  

$$
Gini(S)=1−\sum_{k=1}^K p_{K}^2
$$

- $$k$$: 클래스의 개수.
- $$p_k$$: 클래스 $$k$$에 속할 확률.

{: .box-success}
지니 계수는 기본적으로 **불평등도**를 측정합니다. 값이 0에 가까울수록 한 클래스만 포함된 순수한 노드이며 값이 1에 가까울수록 여러 클래스가 고르게 섞인 불순한 노드입니다.

사실 두 계산 척도는 상황에 따라 사용을 하면 된다고 생각이 듭니다.
- **정보획득량**: 분류 결과의 정밀도를 높이고 싶거나, 분할 기준의 해석 가능성을 중시할 때 유용합니다.
- **지니 계수**: 속도가 중요한 대규모 데이터셋에서 적합하며, 계산 비용을 줄일 수 있습니다.

## 세부적인 계산 예시

자 이제 한가지 예를 들어봅시다.  
A 방법과 B 방법 모두 노드 분리 전에는 Y=0 인 데이터의 수와 Y=1 인 데이터의 수가 모두 40개였습니다.  
A 방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생깁니다.
- 자식 노드 A1은 Y=0 인 데이터가 30개, Y=1 인 데이터가 10개
- 자식 노드 A2은 Y=0 인 데이터가 10개, Y=1 인 데이터가 30개
B 방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생깁니다.
- 자식 노드 B1은 Y=0 인 데이터가 20개, Y=1 인 데이터가 40개
- 자식 노드 B2은 Y=0 인 데이터가 20개, Y=1 인 데이터가 0개  
  
우선 부모 노드의 엔트로피를 계산하면 다음과 같습니다.

$$
H[Y] = -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) = \dfrac{1}{2} + \dfrac{1}{2}  = 1 
$$

A 방법에 대해 정보획득량을 계산하면 다음과 같습니다.

$$
H[Y|X=X_1] = -\dfrac{3}{4}\log_2\left(\dfrac{3}{4}\right) -\dfrac{1}{4}\log_2\left(\dfrac{1}{4}\right) = 0.81 
$$

$$
H[Y|X=X_2] = -\dfrac{1}{4}\log_2\left(\dfrac{1}{4}\right)  -\dfrac{3}{4}\log_2\left(\dfrac{3}{4}\right) = 0.81 
$$

$$
H[Y|X] = \dfrac{1}{2} H[Y|X=X_1] + \dfrac{1}{2} H[Y|X=X_2] = 0.81 
$$

$$
IG = H[Y] - H[Y|X] = 0.19
$$

B 방법에 대해 정보획득량을 계산하면 다음과 같습니다.

$$
 H[Y|X=X_1] = -\dfrac{1}{3}\log_2\left(\dfrac{1}{3}\right) - \dfrac{2}{3}\log_2\left(\dfrac{2}{3}\right) = 0.92 
$$

$$
 H[Y|X=X_2] = 0 
$$

$$
 H[Y|X] = \dfrac{3}{4} H[Y|X=X_1] + \dfrac{1}{4} H[Y|X=X_2] = 0.69 
$$

$$
 IG = H[D] - H[Y|X] = 0.31 
$$

따라서 엔트로피가 작은 B방법이 더 좋은 방법임을 알 수 있습니다.  
그럼 이 방법을 이용해서 어떻게 의사결정나무가 만들어 지는지 알아본다면 아래와 같습니다.  
예를 들어, 다음과 같은 데이터가 있다고 합시다. 

| $$X_1$$ | $$X_2$$ |
| --- | --- |
| 1.4 | 1 |
| 1.1 | 1 |
| 0.2 | 1 |
| 0.8 | 0 |
| -0.1 | 1 |
| -0.5 | 0 |

1. $$X_1$$ 에 대해서 값을 모두 정렬합니다.
2. $$X_1$$ 에 대해서 차례대로 threshhold를 지정하여 Information Gain을 계산합니다..
    - 예를 들어, 가장 먼저 $$X_1$$ 데이터 중 1.4와 1.1 사이를 threshold로 지정합니다.. ⇒ $$X_1$$ < 1.25
    - 그 다음으로 $$X_1$$ 데이터 중 1.1와 0.8 사이를 threshold로 지정합니다.. ⇒ $$X_1$$ < 0.95
    - a와 b를 반복합니다..
    - ⛔ 즉, 값이 변해지는 지점을 모두 threshold를 지정해보고 Information Gain을 구합니다..
  
3. $$X_1$$이 끝이 나면 $$X_2$$도 동일하게 모두 정렬하여 Information Gain을 구합니다..
    - $$X_2$$의 경우에는 1과 0 사이만 Information Gain을 구합니다..  
  
4. 2번과 3번을 모든 변수에 대해서 반복하여 가장 Information Gain이 큰 부분은 부모 노드로 지정합니다..  
5. 위의 과정을 1) MaxDepth만큼 하거나, 2) 순수노드가 나올때 까지 진행합니다.

## 의사결정나무 탐욕 문제

의사결정나무는 각 노드에서 데이터를 분리할 때 현재 상태에서 최적의 기준(즉, 정보 이득(Information Gain) 또는 지니 계수(Gini Index))을 선택합니다.  
이 과정은 탐욕적 접근법에 기반하며, 다음과 같은 한계가 있습니다:  
1. 글로벌 최적해를 찾지 못할 가능성
- 문제: 각 단계에서 국소적으로 최적의 분할 기준을 선택하지만, 전체적으로는 최적의 트리를 구성하지 못할 가능성이 큽니다.
- 초기 몇 번의 분할에서 잘못된 선택을 하면, 이후의 분할이 아무리 잘 이루어져도 최적의 트리를 만들기 어렵습니다. 특정 변수의 중요한 상호작용 효과를 무시한 채 단일 분할 기준만 고려합니다.
- 해결 방법: 앙상블 방법(Random Forest, Gradient Boosting) 등을 사용하여 다수의 트리를 학습하고, 이들의 결과를 종합하여 보다 안정적인 결과를 얻습니다.
2. 다양한 분할 가능성 탐색의 부족
- 문제: 트리는 각 노드에서 한 번의 분할만 수행하며, 다른 가능성은 고려하지 않습니다. 이로 인해, 보다 나은 분할 조합을 놓칠 가능성이 있습니다.
- 원인: 트리는 깊이를 내려가면서 한 번 선택한 분할을 고수하므로, 뒤로 돌아가 재조정할 수 없습니다.
- 프루닝(Pruning): 트리가 너무 깊어지지 않도록 가지치기를 통해 복잡도를 줄이거나 최소 샘플 수, 최대 깊이 등을 제어하여 과도한 분할을 방지할 수 있습니다. 
3. 데이터 균형 및 중요도 왜곡
- 문제: 탐욕적 알고리즘은 데이터의 분포에 따라 편향된 분할을 할 가능성이 있습니다. 특히, 연속형 변수와 범주형 변수가 혼합된 데이터에서 문제가 됩니다.
- 연속형 변수는 작은 변동에도 민감하게 반응하여 중요도가 높게 계산될 수 있습니다.
- 범주형 변수는 고르게 분포하지 않으면 분할 기준에서 무시될 수 있습니다.
- 데이터 스케일링(표준화) 또는 적절한 변수 전처리를 통해 변수를 균형 있게 반영하도록 유도하거나 혹은 범주형에 맞는 의사결정나무 (Chi-square automatic interaction detection)를 사용할 수 있습니다. 

### 왜 탐욕적 접근법이 여전히 사용될까?
탐욕적 접근법은 단점이 있지만, 의사결정나무에서 다음과 같은 이유로 널리 사용됩니다:

- 단순함과 속도: 각 노드에서 최적의 기준을 선택하므로 계산이 간단하고 빠릅니다.
- 해석 가능성: 국소적으로 최적의 선택을 반복하는 구조가 트리를 직관적으로 해석하기 쉽게 만듭니다.
- 실용성: 탐욕적 접근이 데이터 분류와 예측에서 종종 충분히 좋은 성능을 보입니다.


# Reference 
- https://datascienceschool.net/03%20machine%20learning/12.01%20%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4.html
- https://ratsgo.github.io/machine%20learning/2017/03/26/tree/